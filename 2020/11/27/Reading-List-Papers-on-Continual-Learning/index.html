<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/assets/favicon/favicon_01/favicon-16x16.png?v=2.3.0" type="image/png" sizes="16x16"><link rel="icon" href="/assets/favicon/favicon_01/favicon-32x32.png?v=2.3.0" type="image/png" sizes="32x32"><meta name="description" content="1982        Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.                     1986        J. C. Schlimmer">
<meta property="og:type" content="article">
<meta property="og:title" content="Reading List: Papers on Continual Learning">
<meta property="og:url" content="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/index.html">
<meta property="og:site_name" content="Hello, Stranger">
<meta property="og:description" content="1982        Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.                     1986        J. C. Schlimmer">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-11-27T00:47:15.000Z">
<meta property="article:modified_time" content="2021-01-30T10:03:19.139Z">
<meta property="article:author" content="liuyang-70d12d">
<meta property="article:tag" content="Continual Learning">
<meta name="twitter:card" content="summary"><title>Reading List: Papers on Continual Learning | Hello, Stranger</title><link ref="canonical" href="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.3.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.2.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Reading List: Papers on Continual Learning</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2020-11-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-01-30</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">Visited</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"> <a id="more"></a> 

        <h4 id="1982">
          <a href="#1982" class="heading-link"><i class="fas fa-link"></i></a>1982</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.</font></p>

        <h4 id="1986">
          <a href="#1986" class="heading-link"><i class="fas fa-link"></i></a>1986</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">J. C. Schlimmer and D. H. Fisher. A case study of incremental concept induction. In AAAI, 1986.</font></p>

        <h4 id="1989">
          <a href="#1989" class="heading-link"><i class="fas fa-link"></i></a>1989</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.</font></p>

        <h4 id="1990">
          <a href="#1990" class="heading-link"><i class="fas fa-link"></i></a>1990</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97 2:285–308, 1990.</font></p>

        <h4 id="1995">
          <a href="#1995" class="heading-link"><i class="fas fa-link"></i></a>1995</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">S. Thrun. A lifelong learning perspective for mobile robot control. In V. Graefe (ed.), Intelligent Robots and Systems. Elsevier, 1995.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123–146, 1995.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Thrun, S. and Mitchell, T. Lifelong robot learning. Robotics and Autonomous Systems, 15:25–46, 1995.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="1996">
          <a href="#1996" class="heading-link"><i class="fas fa-link"></i></a>1996</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">S. Thrun. Is learning the n-th thing any easier than learning the first? In NIPS, 1996.</font></p>

        <h4 id="1997">
          <a href="#1997" class="heading-link"><i class="fas fa-link"></i></a>1997</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77–104, 1997.</font></p>

        <h4 id="1998">
          <a href="#1998" class="heading-link"><i class="fas fa-link"></i></a>1998</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mark B. Ring. Child: A first step towards continual learning. In Learning to Learn, 1998.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="1999">
          <a href="#1999" class="heading-link"><i class="fas fa-link"></i></a>1999</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">French, R.M.: Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3(4), 128–135 (1999)</font></p>

        <h4 id="2000">
          <a href="#2000" class="heading-link"><i class="fas fa-link"></i></a>2000</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In NIPS, 2000.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2001">
          <a href="#2001" class="heading-link"><i class="fas fa-link"></i></a>2001</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">R. Polikar, L. Upda, S. S. Upda, and V. Honavar. Learn++: An incremental learning algorithm for supervised neural networks. IEEE Trans. Systems, Man, and Cybernetics, Part C, 31(4):497–508, 2001.</font></p>

        <h4 id="2002">
          <a href="#2002" class="heading-link"><i class="fas fa-link"></i></a>2002</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">D. L. Silver and R. E. Mercer, “The task rehearsal method of life-long learning: Overcoming impoverished data,” in Conference of the Canadian Society for Computational Studies of Intelligence. Springer, 2002, pp. 90–101.</font></p>

        <h4 id="2005">
          <a href="#2005" class="heading-link"><i class="fas fa-link"></i></a>2005</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">O.-M. Moe-Helgesen and H. Stranden. Catastophic forgetting in neural networks. Technical report, Norwegian University of Science and Technology (NTNU), 2005.</font></p>

        <h4 id="2012">
          <a href="#2012" class="heading-link"><i class="fas fa-link"></i></a>2012</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Zhou, Guanyu, Kihyuk Sohn, and Honglak Lee. “Online incremental feature learning with denoising autoencoders.” <em>Artificial intelligence and statistics</em>. 2012. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. In Proceedings of the International Conference on Machine Learning (ICML), 2012.</font></p>

        <h4 id="2013">
          <a href="#2013" class="heading-link"><i class="fas fa-link"></i></a>2013</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, “An empirical investigation of catastrophic forgetting in gradient-based neural networks,” arXiv preprint arXiv:1312.6211, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">I. Kuzborskij, F. Orabona, and B. Caputo. From n to n + 1: Multiclass transfer incremental learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algorithms. In AAAI Spring Symposium: Lifelong Machine Learning, pages 49–55. Citeseer, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Srivastava, Rupesh K, Masci, Jonathan, Kazerounian, Sohrob, Gomez, Faustino, and Schmidhuber, Juergen. Compete to Compute. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 2310–2318. Curran Associates, Inc., 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Paul Ruvolo and Eric Eaton. Ella: An efficient lifelong learning algorithm. In Proceedings of the International Conference on Machine Learning (ICML), 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. PAMI, 35(11):2624–2637, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen Schmidhuber. Compete to compute. In Advances in neural information processing systems, pages 2310–2318, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2014">
          <a href="#2014" class="heading-link"><i class="fas fa-link"></i></a>2014</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">M. Ristin, M. Guillaumin, J. Gall, and L. Van Gool. Incremental learning of NCM forests for large-scale image classification. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio. An empirical investigation of catastrophic forgeting in gradient-based neural networks. In International Conference on Learning Representations (ICLR), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">T. Xiao, J. Zhang, K. Yang, Y. Peng, and Z. Zhang. Error-driven incremental learning in deep convolutional neural network for large-scale image classification. In International Conference on Multimedia (ACM MM), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features off-theshelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 806–813, 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference in Machine Learning (ICML), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320–3328, 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Zhiyuan Chen and Bing Liu. 2014. Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data. In ICML.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2015">
          <a href="#2015" class="heading-link"><i class="fas fa-link"></i></a>2015</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">A. Pentina and C. H. Lampert. Lifelong learning with non-iid tasks. In Advances in Neural Information Processing Systems, pages 1540–1548, 2015.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Zhiyuan Chen, Nianzu Ma, and Bing Liu. 2015. Lifelong learning for sentiment classification. In ACL. 750–756.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2016">
          <a href="#2016" class="heading-link"><i class="fas fa-link"></i></a>2016</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/05/Paper-Note-Learning-without-Forgetting/"><font face="Adobe Caslon Pro"> Zhizhong Li, et al. “Learning without forgetting.” ECCV. 2016</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/17/Paper-Note-Progressive-Neural-Networks/"><font face="Adobe Caslon Pro"> Andrei A. Rusu, et al. “Progressive neural networks.” arXiv:1606.04671. 2016. </font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Sang-Woo Lee, Chung-Yeon Lee, Dong Hyun Kwak, Jiwon Kim, Jeonghee Kim, and Byoung-Tak Zhang. Dual-memory deep learning architectures for lifelong learning of everyday human behaviors. In Twenty-Fifth International Joint Conference on Artificial Intelligencee, pages 1669–1675, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">A. Gepperth and C. Karaoguz, “A bio-inspired incremental learning architecture for applied perceptual problems,” Cognitive Computation, vol. 8, no. 5, pp. 924–934, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Kieran Milan, Joel Veness, James Kirkpatrick, Michael Bowling, Anna Koop, and Demis Hassabis. The forget-me-not process. In NeurIPS, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16, pp. 1620–1626. AAAI Press, 2016. ISBN 978-1-57735-770-4.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo. Less-forgetting Learning in Deep Neural Networks. arXiv:1607.00122 [cs], July 2016. arXiv: 1607.00122.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2017">
          <a href="#2017" class="heading-link"><i class="fas fa-link"></i></a>2017</h4>
      
<p>√  <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/11/30/Paper-Note-iCaRL-Incremental-Classifier-and-Representation-Learning/"> Sylvestre-Alvise Rebuffi, et al. icarl: Incremental classifier and representation learning. CVPR. 2017</a></font></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/06/Paper-Note-Encoder-Based-Lifelong-Learning/"><font face="Adobe Caslon Pro">  Amal Rannen, et al. Encoder based lifelong learning. ICCV. 2017</font></a></p>
<p>√ <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/12/15/Paper-Note-Incremental-Learning-of-Object-Detectors-Without-Catastrophic-Forgetting/">  Konstantin Shmelkov, et al. Incremental learning of object detectors without catastrophic forgetting. ICCV. 2017</a></font></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/08/Paper-Note-Continual-Learning-Through-Synaptic-Intelligence/"><font face="Adobe Caslon Pro">Friedemann Zenke, et al. Continual Learning Through Synaptic Intelligence. ICML. 2017</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/02/Paper-Note-Gradient-Episodic-Memory-for-Continual-Learning/"><font face="Adobe Caslon Pro"> David Lopez-Paz, et al.  “Gradient episodic memory for continual learning.” NIPS. 2017. </font></a></p>
<p>√ <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/12/16/Paper-Note-Overcoming-Catastrophic-Forgetting-by-Incremental-Moment-Matching/"> Sang-Woo Lee, et al. “Overcoming catastrophic forgetting by incremental moment matching.” NIPS. 2017. </a></font></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/07/Paper-Note-Overcoming-Catastrophic-Forgetting-in-Neural-Networks/"><font face="Adobe Caslon Pro"> James Kirkpatrick, et al. “Overcoming catastrophic forgetting in neural networks.” Proceedings of the National Academy of Sciences of the United States of America. 2017</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/"><font face="Adobe Caslon Pro">Chrisantha Fernando, et al. “Pathnet: Evolution channels gradient descent in super neural networks”. arXiv:1701.08734. 2017.</font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">R. Aljundi, P. Chakravarty, and T. Tuytelaars, “Expert gate: Lifelong learning with a network of experts,” in CVPR, 2017, pp. 3366–3375.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, pp. 506–516, 2017</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yuchun Fang, Zhengyan Ma, Zhaoxiang Zhang, Xu-Yao Zhang, and Xiang Bai. Dynamic multi-task learning with convolutional neural network. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2017.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increasing<br>
model capacity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2471–2480, 2017.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2018">
          <a href="#2018" class="heading-link"><i class="fas fa-link"></i></a>2018</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/13/Paper-Note-PackNet-Adding-Multiple-Tasks-to-a-Single-Network-by-Iterative-Pruning/"><font face="Adobe Caslon Pro"> Arun Mallya, et al. “Packnet: Adding multiple tasks to a single network by iterative pruning.” CVPR. 2018. </font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/09/Paper-Note-Memory-Aware-Synapses-Learning-what-not-to-forget/"><font face="Adobe Caslon Pro">Rahaf Aljundi, et al. Memory aware synapses: Learning what (not) to forget. ECCV. 2018.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/18/Paper-Note-Lifelong-Learning-with-Dynamically-Expandable-Networks/"><font face="Adobe Caslon Pro">Jaehong Yoon, et al. Lifelong learning with dynamically expandable networks. ICLR. 2018.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/30/Paper-Note-Overcoming-Catastrophic-Forgetting-with-Hard-Attention-to-the-Task/"><font face="Adobe Caslon Pro"> Joan Serra, et al. “Overcoming Catastrophic Forgetting with Hard Attention to the Task”. ICML. 2018. </font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/23/Paper-Note-Reinforced-Continual-Learning/"><font face="Adobe Caslon Pro">  Ju Xu, et al. Reinforced continual learning. NIPS. 2018</font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. ICLR, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Thirty-second AAAI conference on artificial intelligence, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yen-Chang Hsu, Yen-Cheng Liu, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Z. Chen and B. Liu, “Lifelong machine learning,” Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 12, no. 3, pp. 1–207, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">S. Farquhar and Y. Gal, “Towards robust evaluations of continual learning,” arXiv preprint arXiv:1805.09733, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress &amp; compress: A scalable framework for continual learning. In ICML, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Continual reinforcement learning with complex synapses. In ICML, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In AAAI, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mohammad Rostami, Soheil Kolouri, Kyungnam Kim, and Eric Eaton. Multi-agent distributed lifelong learning for collective knowledge acquisition. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mallya, A. and Lazebnik, S. Piggyback: Adding multiple tasks to a single, fixed network by learning to mask. arXiv preprint arXiv:1801.06519, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Efficient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8119–8127, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Mancini, Massimiliano, et al. “Adding new tasks to a single network with weight transformations using binary masks.” <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>. 2018. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selfless sequential learning. In International Conference on Learning Representations (ICLR), 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ronald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. In International Conference on Learning Representations (ICLR), 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532–547, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Amir Rosenfeld and John K Tsotsos. 2018. Incremental learning through deep adaptation. IEEE transactions on pattern analysis and machine intelligence (2018).</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Xu He and Herbert Jaeger. 2018. Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation. In ICLR.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Nicolas Y. Masse, Gregory D. Grant, and David J. Freedman. 2018. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. Proc. Natl. Acad. Sci. U.S.A. (2018).</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Chenshen Wu, Luis Herranz, Xialei Liu, Joost van de Weijer, Bogdan Raducanu, et al. 2018. Memory replay GANs: Learning to generate new categories without forgetting. In NIPS.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pp. 3738–3748, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Nicolas Y. Masse, Gregory D. Grant, and David J. Freedman. Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization. Proceedings of the National Academy of Sciences of the United States of America, 115 44, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pp. 3302–3309, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Alessandro Achille, Tom Eccles, Loic Matthey, Chris Burgess, Nicholas Watters, Alexander Lerchner, and Irina Higgins. Life-long disentangled representation learning with cross-domain latent homologies. In Advances in Neural Information Processing Systems 31 (NeurIPS-18), pp. 9873–9883, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. Houdini: Lifelong learning as program synthesis. In Advances in Neural Information Processing Systems 31 (NeurIPS-18), pp. 8687–8698, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Amir Rosenfeld and John K Tsotsos. Incremental learning through deep adaptation. IEEE transactions on pattern analysis and machine intelligence, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">FranciscoMCastro, Manuel J Mar´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 233–248, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen. Exemplar-supported generative reproduction for class incremental learning. In British Machine Vision Conference, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2019">
          <a href="#2019" class="heading-link"><i class="fas fa-link"></i></a>2019</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/02/Paper-Note-Efficient-Lifelong-Learning-with-A-GEM/"><font face="Adobe Caslon Pro">Arslan Chaudhry, et al. Efficient lifelong learning with a-gem. ICLR. 2019.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/23/Paper-Note-Learn-to-Grow-A-Continual-Structure-Learning-Framework-for-Overcoming-Catastrophic-Forgetting/"><font face="Adobe Caslon Pro"> Xilai Li, et al. Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting. ICML. 2019</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/01/Paper-Note-Experience-Replay-for-Continual-Learning/"><font face="Adobe Caslon Pro">David Rolnick, et al. Experience replay for continual learning. NIPS. 2019</font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember: A synaptic plasticity driven framework for continual learning. In IEEE International Conference on Computer Vision and Pattern Recognition, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato, “Continual learning with tiny episodic memories,” arXiv preprint arXiv:1902.10486, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S. Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. ArXiv, abs/2002.08165, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthias Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale Leonardis, Gregory G.Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. ArXiv, abs/1909.08383, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward understanding catastrophic forgetting in continual learning. arXiv preprint arXiv:1908.01091, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. ArXiv, abs/1910.07104, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey">G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual lifelong learning with neural networks: A review,” Neural Networks, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Pfülb, Benedikt, and Alexander Gepperth. “A comprehensive, application-oriented study of catastrophic forgetting in dnns.” <em>arXiv preprint arXiv:1905.08101</em> (2019). </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Policy consolidation for continual reinforcement learning. In ICML, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Aljundi, Rahaf, et al. “Gradient based sample selection for online continual learning.” <em>Advances in Neural Information Processing Systems</em>. 2019. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. In Advances in Neural Information Processing Systems (NIPS), 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon. Uncertainty-based continual learning with adaptive regularization. In Advances in Neural Information Processing Systems (NeurIPS), pages 4394–4404, 2019.</font></p>
<p>★<font face="Adobe Caslon Pro" color="grey">Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual learning via neural pruning. Advances in Neural Information Processing Systems (NeurIPS) Workshop, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. 2019. Continual lifelong learning with neural networks: A review. Neural Networks (2019).</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao, Jinwen Ma, Dongyan Zhao, and Rui Yan. 2019. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation. In ICLR.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. 2019. Learning without Memorizing. In CVPR.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Khurram Javed and Martha White. 2019. Meta-Learning Representations for Continual Learning. In NeurIPS-2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz, and Khan Ling Shao. 2019. Random Path Selection for Incremental Learning. In NeurIPS.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mohammad Rostami, Soheil Kolouri, and Praveen K. Pilly. 2019. Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay. In IJCAI.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> van de Ven, Gido M., and Andreas S. Tolias. “Three scenarios for continual learning.” <em>arXiv preprint arXiv:1904.07734</em> (2019). </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S. Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. ArXiv, abs/2002.08165, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ghassen Jerfel, Erin Grant, Thomas L. Griffiths, and Katherine A. Heller. Reconciling meta-learning and continual learning with online mixtures of tasks. In NeurIPS, 2019.</font></p>
<p>★<font face="Adobe Caslon Pro" color="grey">Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell. Continual unsupervised representation learning. In Advances in Neural Information Processing Systems, pp. 7645–7655, 2019.</font></p>
<p>★<font face="Adobe Caslon Pro" color="grey">Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.08671, 2019.</font></p>
<p>★<font face="Adobe Caslon Pro" color="grey">Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan Pascanu. Task agnostic continual learning via meta learning. arXiv preprint arXiv:1906.05201, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Rajasegaran J, Hayat M, Khan S, et al. Random path selection for incremental learning[J]. Advances in Neural Information Processing Systems, 2019. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Eden Belouadah and Adrian Popescu. Il2m: Class incremental learning with dual memory. In The IEEE International Conference on Computer Vision (ICCV), October 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yue Wu, et al. Large scale incremental learning. CVPR. 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Prithviraj Dhar, et al. Learning without memorizing. CVPR. 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2020">
          <a href="#2020" class="heading-link"><i class="fas fa-link"></i></a>2020</h4>
      
<p>√ <a href="https://hello-liuyang.com/2021/01/11/Paper-Note-Conditional-Channel-Gated-Networks-for-Task-Aware-Continual-Learning/"><font face="Adobe Caslon Pro"> Davide Abati, et al. “Conditional Channel Gated Networks for Task-Aware Continual Learning.” CVPR. 2020. </font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/24/Paper-Note-Scalable-and-Order-robust-Continual-Learning-with-Additive-Parameter-Decomposition/"><font face="Adobe Caslon Pro">Jaehong Yoon, et al. Scalable and order-robust continual learning with additive parameter decomposition. ICLR. 2020.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/29/Paper-Note-Continual-Learning-with-Node-Importance-based-Adaptive-Group-Sparse-Regularization/"><font face="Adobe Caslon Pro">  Sangwon Jung, et al. Continual Learning with Node-Importance based Adaptive Group Sparse Regularization. NIPS. 2020.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/30/Paper-Note-Continual-Learning-of-a-Mixed-Sequence-of-Similar-and-Dissimilar-Tasks/"><font face="Adobe Caslon Pro"> Zixuan Ke, et al. “Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks.” NIPS. 2020. </font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Saha, Gobinda, et al. “Structured Compression and Sharing of Representational Space for Continual Learning.” <em>arXiv preprint arXiv:2001.08650</em> (2020). </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Dong Yin, Mehrdad Farajtabar, and Ang Li. SOLA: Continual learning with second-order loss approximation. arXiv preprint arXiv:2006.10974, 2020.</font></p>
<p>★ <font face="Adobe Caslon Pro" color="grey">Seyed-Iman Mirzadeh, Mehrdad Farajtabar, and Hassan Ghasemzadeh. Dropout as an implicit gating mechanism for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 232–233, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, and Ali Farhadi. In the wild: From ml models to pragmatic ml systems. ArXiv, abs/2007.02519, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yunhui Guo, Mingrui Liu, Tianbao Yang, and T. Rosing. Improved schemes for episodic memorybased lifelong learning. In Advances in Neural Information Processing Systems 33, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah. 2020. iTAML: An Incremental Task-Agnostic Meta-learning Approach. In CVPR. 13588–13597.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin F Grewe. 2020. Continual learning with hypernetworks. In ICLR.</font></p>
<p>★ <font face="Adobe Caslon Pro" color="grey">Mitchell Wortsman, V. Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, J. Yosinski, and Ali Farhadi. Supermasks in superposition. ArXiv, abs/2006.14769, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Tyler L Hayes and Christopher Kanan. Lifelong machine learning with deep streaming linear discriminant analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 220–221, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu, Andrew D Bagdanov, Shangling Jui, and Joost van de Weijer. Generative feature replay for class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 226–227, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ghada Sokar, et al. Spacenet: Make free space for continual learning.  Neurocomputing. 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Sayna Ebrahimi, et al. Adversarial continual learning. ECCV. 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Xiaoyu Tao, et al. Fewshot class-incremental learning. CVPR. 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2021">
          <a href="#2021" class="heading-link"><i class="fas fa-link"></i></a>2021</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/17/Paper-Note-Gradient-Projection-Memory-for-Continual-Learning/"><font face="Adobe Caslon Pro">Gobinda Saha, et al. Gradient Projection Memory for Continual Learning. ICLR 2021</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/11/26/Paper-Note-Linear-Mode-Connectivity-in-Multitask-and-Continual-Learning/"><font face="Adobe Caslon Pro">Seyed Iman Mirzadeh, et al. Linear Mode Connectivity in Multitask and Continual Learning. ICLR 2021</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2021/01/25/Paper-Note-Lifelong-Learning-of-Compositional-Structures/"><font face="Adobe Caslon Pro">Jorge A Mendez, et al. Lifelong Learning of Compositional Structures. ICLR. 2021</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2021/01/26/Paper-Note-Long-Live-the-Lottery-The-Existence-of-Winning-Tickets-in-Lifelong-Learning/"><font face="Adobe Caslon Pro">Tianlong Chen, et al. Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning. ICLR. 2021</font></a></p>
<p><font face="Adobe Caslon Pro">Kevin Lu, et al. Reset-Free Lifelong Learning with Skill-Space Planning. ICLR. 2021</font></p>
<ul>
<li><font size="2">强化学习相关的，研究的是持续强化学习</font></li>
</ul>
<p><font face="Adobe Caslon Pro" color="grey">Sayna Ebrahimi, et al. Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting. ICLR. 2021</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Kuilin Chen, et al. Incremental few-shot learning via vector quantization in deep embedded space. ICLR. 2021</font></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="https://hello-liuyang.com">liuyang-70d12d</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/">https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hello-liuyang.com/tags/Continual-Learning/">Continual Learning</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2020/11/27/Paper-Note-A-continual-learning-survey-Defying-forgetting-in-classification-tasks/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Paper Note: A continual learning survey: Defying forgetting in classification tasks</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2020/11/26/Paper-Note-Linear-Mode-Connectivity-in-Multitask-and-Continual-Learning/"><span class="paginator-prev__text">Paper Note: Linear Mode Connectivity in Multitask and Continual Learning</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1982"><span class="toc-number">1.</span> <span class="toc-text">
          1982</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1986"><span class="toc-number">2.</span> <span class="toc-text">
          1986</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1989"><span class="toc-number">3.</span> <span class="toc-text">
          1989</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1990"><span class="toc-number">4.</span> <span class="toc-text">
          1990</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1995"><span class="toc-number">5.</span> <span class="toc-text">
          1995</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1996"><span class="toc-number">6.</span> <span class="toc-text">
          1996</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1997"><span class="toc-number">7.</span> <span class="toc-text">
          1997</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1998"><span class="toc-number">8.</span> <span class="toc-text">
          1998</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1999"><span class="toc-number">9.</span> <span class="toc-text">
          1999</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2000"><span class="toc-number">10.</span> <span class="toc-text">
          2000</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2001"><span class="toc-number">11.</span> <span class="toc-text">
          2001</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2002"><span class="toc-number">12.</span> <span class="toc-text">
          2002</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2005"><span class="toc-number">13.</span> <span class="toc-text">
          2005</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2012"><span class="toc-number">14.</span> <span class="toc-text">
          2012</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2013"><span class="toc-number">15.</span> <span class="toc-text">
          2013</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2014"><span class="toc-number">16.</span> <span class="toc-text">
          2014</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2015"><span class="toc-number">17.</span> <span class="toc-text">
          2015</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2016"><span class="toc-number">18.</span> <span class="toc-text">
          2016</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2017"><span class="toc-number">19.</span> <span class="toc-text">
          2017</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2018"><span class="toc-number">20.</span> <span class="toc-text">
          2018</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2019"><span class="toc-number">21.</span> <span class="toc-text">
          2019</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2020"><span class="toc-number">22.</span> <span class="toc-text">
          2020</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2021"><span class="toc-number">23.</span> <span class="toc-text">
          2021</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/avatar/avatar.png" alt="avatar"></div></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="mailto:ly_liuyang19@qq.com" target="_blank" rel="noopener" data-popover="social.mail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">Email</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">49</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">19</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>liuyang-70d12d</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.2.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.3.0</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">Visitors</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">Views</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now = new Date();
function createtime() {
var grt= new Date("11/09/2020 00:00:00");
now.setTime(now.getTime()+250);
days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
document.getElementById("timeDate").innerHTML = "我已来到这个世界 "+dnum+" 天 ";
document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
}
setInterval("createtime()",250);</script></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css"><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="/js/utils.js?v=2.3.0"></script><script src="/js/stun-boot.js?v=2.3.0"></script><script src="/js/scroll.js?v=2.3.0"></script><script src="/js/header.js?v=2.3.0"></script><script src="/js/sidebar.js?v=2.3.0"></script></body></html>