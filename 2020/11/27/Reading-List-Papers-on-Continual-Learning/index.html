<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/assets/favicon/favicon_01/favicon-16x16.png?v=2.3.0" type="image/png" sizes="16x16"><link rel="icon" href="/assets/favicon/favicon_01/favicon-32x32.png?v=2.3.0" type="image/png" sizes="32x32"><meta name="description" content="1982        Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.                     1986        J. C. Schlimmer">
<meta property="og:type" content="article">
<meta property="og:title" content="Reading List: Papers on Continual Learning">
<meta property="og:url" content="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/index.html">
<meta property="og:site_name" content="Hello, Stranger">
<meta property="og:description" content="1982        Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.                     1986        J. C. Schlimmer">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-11-27T00:47:15.000Z">
<meta property="article:modified_time" content="2020-12-20T03:13:13.863Z">
<meta property="article:author" content="liuyang-70d12d">
<meta property="article:tag" content="Continual Learning">
<meta name="twitter:card" content="summary"><title>Reading List: Papers on Continual Learning | Hello, Stranger</title><link ref="canonical" href="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.3.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.2.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Reading List: Papers on Continual Learning</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2020-11-27</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2020-12-20</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">Visited</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"> <a id="more"></a> 

        <h4 id="1982">
          <a href="#1982" class="heading-link"><i class="fas fa-link"></i></a>1982</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Stephen Grossberg. How does a brain build a cognitive code? In Studies of mind and brain, pages 1–52. Springer, 1982.</font></p>

        <h4 id="1986">
          <a href="#1986" class="heading-link"><i class="fas fa-link"></i></a>1986</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">J. C. Schlimmer and D. H. Fisher. A case study of incremental concept induction. In AAAI, 1986.</font></p>

        <h4 id="1989">
          <a href="#1989" class="heading-link"><i class="fas fa-link"></i></a>1989</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.</font></p>

        <h4 id="1990">
          <a href="#1990" class="heading-link"><i class="fas fa-link"></i></a>1990</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 97 2:285–308, 1990.</font></p>

        <h4 id="1995">
          <a href="#1995" class="heading-link"><i class="fas fa-link"></i></a>1995</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">S. Thrun. A lifelong learning perspective for mobile robot control. In V. Graefe (ed.), Intelligent Robots and Systems. Elsevier, 1995.</font></p>

        <h4 id="1996">
          <a href="#1996" class="heading-link"><i class="fas fa-link"></i></a>1996</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">S. Thrun. Is learning the n-th thing any easier than learning the first? In NIPS, 1996.</font></p>

        <h4 id="1997">
          <a href="#1997" class="heading-link"><i class="fas fa-link"></i></a>1997</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Mark B Ring. Child: A first step towards continual learning. Machine Learning, 28(1):77–104, 1997.</font></p>

        <h4 id="1998">
          <a href="#1998" class="heading-link"><i class="fas fa-link"></i></a>1998</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Sebastian Thrun. Lifelong learning algorithms. In Learning to learn, pp. 181–209. Springer, 1998.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mark B. Ring. Child: A first step towards continual learning. In Learning to Learn, 1998.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="1999">
          <a href="#1999" class="heading-link"><i class="fas fa-link"></i></a>1999</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">French, R.M.: Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3(4), 128–135 (1999)</font></p>

        <h4 id="2000">
          <a href="#2000" class="heading-link"><i class="fas fa-link"></i></a>2000</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In NIPS, 2000.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2001">
          <a href="#2001" class="heading-link"><i class="fas fa-link"></i></a>2001</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">R. Polikar, L. Upda, S. S. Upda, and V. Honavar. Learn++: An incremental learning algorithm for supervised neural networks. IEEE Trans. Systems, Man, and Cybernetics, Part C, 31(4):497–508, 2001.</font></p>

        <h4 id="2002">
          <a href="#2002" class="heading-link"><i class="fas fa-link"></i></a>2002</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">D. L. Silver and R. E. Mercer, “The task rehearsal method of life-long learning: Overcoming impoverished data,” in Conference of the Canadian Society for Computational Studies of Intelligence. Springer, 2002, pp. 90–101.</font></p>

        <h4 id="2005">
          <a href="#2005" class="heading-link"><i class="fas fa-link"></i></a>2005</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">O.-M. Moe-Helgesen and H. Stranden. Catastophic forgetting in neural networks. Technical report, Norwegian University of Science and Technology (NTNU), 2005.</font></p>

        <h4 id="2012">
          <a href="#2012" class="heading-link"><i class="fas fa-link"></i></a>2012</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Zhou, Guanyu, Kihyuk Sohn, and Honglak Lee. “Online incremental feature learning with denoising autoencoders.” <em>Artificial intelligence and statistics</em>. 2012. </font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2013">
          <a href="#2013" class="heading-link"><i class="fas fa-link"></i></a>2013</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, “An empirical investigation of catastrophic forgetting in gradient-based neural networks,” arXiv preprint arXiv:1312.6211, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">I. Kuzborskij, F. Orabona, and B. Caputo. From n to n + 1: Multiclass transfer incremental learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">D. L. Silver, Q. Yang, and L. Li. Lifelong machine learning systems: Beyond learning algorithms. In AAAI Spring Symposium: Lifelong Machine Learning, pages 49–55. Citeseer, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Srivastava, Rupesh K, Masci, Jonathan, Kazerounian, Sohrob, Gomez, Faustino, and Schmidhuber, Juergen. Compete to Compute. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 2310–2318. Curran Associates, Inc., 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Paul Ruvolo and Eric Eaton. Ella: An efficient lifelong learning algorithm. In Proceedings of the International Conference on Machine Learning (ICML), 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. PAMI, 35(11):2624–2637, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen Schmidhuber. Compete to compute. In Advances in neural information processing systems, pages 2310–2318, 2013.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2014">
          <a href="#2014" class="heading-link"><i class="fas fa-link"></i></a>2014</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">M. Ristin, M. Guillaumin, J. Gall, and L. Van Gool. Incremental learning of NCM forests for large-scale image classification. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio. An empirical investigation of catastrophic forgeting in gradient-based neural networks. In International Conference on Learning Representations (ICLR), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">T. Xiao, J. Zhang, K. Yang, Y. Peng, and Z. Zhang. Error-driven incremental learning in deep convolutional neural network for large-scale image classification. In International Conference on Multimedia (ACM MM), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Razavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and Carlsson, Stefan. Cnn features off-theshelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 806–813, 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference in Machine Learning (ICML), 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320–3328, 2014.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2015">
          <a href="#2015" class="heading-link"><i class="fas fa-link"></i></a>2015</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">A. Pentina and C. H. Lampert. Lifelong learning with non-iid tasks. In Advances in Neural Information Processing Systems, pages 1540–1548, 2015.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2016">
          <a href="#2016" class="heading-link"><i class="fas fa-link"></i></a>2016</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/05/Paper-Note-Learning-without-Forgetting/"><font face="Adobe Caslon Pro">Z. Li and D. Hoiem. Learning without forgetting. In European Conference on Computer Vision (ECCV), 2016.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/17/Paper-Note-Progressive-Neural-Networks/"><font face="Adobe Caslon Pro">Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.</font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">Sang-Woo Lee, Chung-Yeon Lee, Dong Hyun Kwak, Jiwon Kim, Jeonghee Kim, and Byoung-Tak Zhang. Dual-memory deep learning architectures for lifelong learning of everyday human behaviors. In Twenty-Fifth International Joint Conference on Artificial Intelligencee, pages 1669–1675, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">A. Gepperth and C. Karaoguz, “A bio-inspired incremental learning architecture for applied perceptual problems,” Cognitive Computation, vol. 8, no. 5, pp. 924–934, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Kieran Milan, Joel Veness, James Kirkpatrick, Michael Bowling, Anna Koop, and Demis Hassabis. The forget-me-not process. In NeurIPS, 2016.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16, pp. 1620–1626. AAAI Press, 2016. ISBN 978-1-57735-770-4.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo. Less-forgetting Learning in Deep Neural Networks. arXiv:1607.00122 [cs], July 2016. arXiv: 1607.00122.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2017">
          <a href="#2017" class="heading-link"><i class="fas fa-link"></i></a>2017</h4>
      
<p>√  <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/11/30/Paper-Note-iCaRL-Incremental-Classifier-and-Representation-Learning/">S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl: Incremental classifier and representation learning,” in CVPR, 2017, pp. 2001–2010.</a></font></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/02/Paper-Note-Gradient-Episodic-Memory-for-Continual-Learning/"><font face="Adobe Caslon Pro">David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/06/Paper-Note-Encoder-Based-Lifelong-Learning/"><font face="Adobe Caslon Pro">A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars, “Encoder based lifelong learning,” in ICCV, 2017, pp. 1320–1328.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/07/Paper-Note-Overcoming-Catastrophic-Forgetting-in-Neural-Networks/"><font face="Adobe Caslon Pro">James N Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, and et. al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 114 13:3521–3526, 2017.</font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/08/Paper-Note-Continual-Learning-Through-Synaptic-Intelligence/"><font face="Adobe Caslon Pro">Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3987–3995. JMLR, 2017.</font></a></p>
<p>√ <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/12/15/Paper-Note-Incremental-Learning-of-Object-Detectors-Without-Catastrophic-Forgetting/">Shmelkov, K., Schmid, C., Alahari, K.: Incremental learning of object detectors without catastrophic forgetting. In: The IEEE International Conference on Computer Vision (ICCV) (2017)</a></font></p>
<p>√ <font face="Adobe Caslon Pro"><a href="https://hello-liuyang.com/2020/12/16/Paper-Note-Overcoming-Catastrophic-Forgetting-by-Incremental-Moment-Matching/">S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, “Overcoming catastrophic forgetting by incremental moment matching,” in NeurIPS, 2017, pp. 4652–4662.</a></font></p>
<p><font face="Adobe Caslon Pro" color="grey">Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">R. Aljundi, P. Chakravarty, and T. Tuytelaars, “Expert gate: Lifelong learning with a network of experts,” in CVPR, 2017, pp. 3366–3375.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A.Rusu, A. Pritzel, and D. Wierstra, “Pathnet: Evolution channels gradient descent in super neural networks,” arXiv preprint arXiv:1701.08734, 2017.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2018">
          <a href="#2018" class="heading-link"><i class="fas fa-link"></i></a>2018</h4>
      
<p><font face="Adobe Caslon Pro" color="grey"> J. Serra, Didac Suris, M. Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018. </font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Mallya, Arun, and Svetlana Lazebnik. “Packnet: Adding multiple tasks to a single network by iterative pruning.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2018. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. ICLR, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Thirty-second AAAI conference on artificial intelligence, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Yen-Chang Hsu, Yen-Cheng Liu, and Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Z. Chen and B. Liu, “Lifelong machine learning,” Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 12, no. 3, pp. 1–207, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">S. Farquhar and Y. Gal, “Towards robust evaluations of continual learning,” arXiv preprint arXiv:1805.09733, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress &amp; compress: A scalable framework for continual learning. In ICML, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Continual reinforcement learning with complex synapses. In ICML, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In AAAI, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Processing Systems (NIPS), 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mohammad Rostami, Soheil Kolouri, Kyungnam Kim, and Eric Eaton. Multi-agent distributed lifelong learning for collective knowledge acquisition. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2018.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2019">
          <a href="#2019" class="heading-link"><i class="fas fa-link"></i></a>2019</h4>
      
<p>√ <a href="https://hello-liuyang.com/2020/12/01/Paper-Note-Experience-Replay-for-Continual-Learning/"><font face="Adobe Caslon Pro"> Rolnick, David, et al. “Experience replay for continual learning.” <em>Advances in Neural Information Processing Systems</em>. 2019. </font></a></p>
<p>√ <a href="https://hello-liuyang.com/2020/12/02/Paper-Note-Efficient-Lifelong-Learning-with-A-GEM/"><font face="Adobe Caslon Pro">Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</font></a></p>
<p><font face="Adobe Caslon Pro" color="grey">A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato, “Continual learning with tiny episodic memories,” arXiv preprint arXiv:1902.10486, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S. Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. ArXiv, abs/2002.08165, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthias Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale Leonardis, Gregory G.Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. ArXiv, abs/1909.08383, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto. Toward understanding catastrophic forgetting in continual learning. arXiv preprint arXiv:1908.01091, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. ArXiv, abs/1910.07104, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. arXiv preprint arXiv:1904.00310, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual lifelong learning with neural networks: A review,” Neural Networks, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Pfülb, Benedikt, and Alexander Gepperth. “A comprehensive, application-oriented study of catastrophic forgetting in dnns.” <em>arXiv preprint arXiv:1905.08101</em> (2019). </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Policy consolidation for continual reinforcement learning. In ICML, 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"> Aljundi, Rahaf, et al. “Gradient based sample selection for online continual learning.” <em>Advances in Neural Information Processing Systems</em>. 2019. </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. In Advances in Neural Information Processing Systems (NIPS), 2019.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2020">
          <a href="#2020" class="heading-link"><i class="fas fa-link"></i></a>2020</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Saha, Gobinda, et al. “Structured Compression and Sharing of Representational Space for Continual Learning.” <em>arXiv preprint arXiv:2001.08650</em> (2020). </font></p>
<p><font face="Adobe Caslon Pro" color="grey">Dong Yin, Mehrdad Farajtabar, and Ang Li. SOLA: Continual learning with second-order loss approximation. arXiv preprint arXiv:2006.10974, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Seyed-Iman Mirzadeh, Mehrdad Farajtabar, and Hassan Ghasemzadeh. Dropout as an implicit gating mechanism for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 232–233, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, and Ali Farhadi. In the wild: From ml models to pragmatic ml systems. ArXiv, abs/2007.02519, 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust continual learning with additive parameter decomposition. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey">Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>

        <h4 id="2021">
          <a href="#2021" class="heading-link"><i class="fas fa-link"></i></a>2021</h4>
      
<p><font face="Adobe Caslon Pro" color="grey">Linear Mode Connectivity in Multitask and Continual Learning. ICLR 2021</font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
<p><font face="Adobe Caslon Pro" color="grey"></font></p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="https://hello-liuyang.com">liuyang-70d12d</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/">https://hello-liuyang.com/2020/11/27/Reading-List-Papers-on-Continual-Learning/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hello-liuyang.com/tags/Continual-Learning/">Continual Learning</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2020/11/27/Paper-Note-A-continual-learning-survey-Defying-forgetting-in-classification-tasks/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Paper Note: A continual learning survey: Defying forgetting in classification tasks</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2020/11/26/Paper-Note-Linear-Mode-Connectivity-in-Multitask-and-Continual-Learning/"><span class="paginator-prev__text">Paper Note: Linear Mode Connectivity in Multitask and Continual Learning</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1982"><span class="toc-number">1.</span> <span class="toc-text">
          1982</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1986"><span class="toc-number">2.</span> <span class="toc-text">
          1986</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1989"><span class="toc-number">3.</span> <span class="toc-text">
          1989</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1990"><span class="toc-number">4.</span> <span class="toc-text">
          1990</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1995"><span class="toc-number">5.</span> <span class="toc-text">
          1995</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1996"><span class="toc-number">6.</span> <span class="toc-text">
          1996</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1997"><span class="toc-number">7.</span> <span class="toc-text">
          1997</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1998"><span class="toc-number">8.</span> <span class="toc-text">
          1998</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1999"><span class="toc-number">9.</span> <span class="toc-text">
          1999</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2000"><span class="toc-number">10.</span> <span class="toc-text">
          2000</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2001"><span class="toc-number">11.</span> <span class="toc-text">
          2001</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2002"><span class="toc-number">12.</span> <span class="toc-text">
          2002</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2005"><span class="toc-number">13.</span> <span class="toc-text">
          2005</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2012"><span class="toc-number">14.</span> <span class="toc-text">
          2012</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2013"><span class="toc-number">15.</span> <span class="toc-text">
          2013</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2014"><span class="toc-number">16.</span> <span class="toc-text">
          2014</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2015"><span class="toc-number">17.</span> <span class="toc-text">
          2015</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2016"><span class="toc-number">18.</span> <span class="toc-text">
          2016</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2017"><span class="toc-number">19.</span> <span class="toc-text">
          2017</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2018"><span class="toc-number">20.</span> <span class="toc-text">
          2018</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2019"><span class="toc-number">21.</span> <span class="toc-text">
          2019</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2020"><span class="toc-number">22.</span> <span class="toc-text">
          2020</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2021"><span class="toc-number">23.</span> <span class="toc-text">
          2021</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/avatar/avatar.png" alt="avatar"></div></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="mailto:ly_liuyang19@qq.com" target="_blank" rel="noopener" data-popover="social.mail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">Email</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">32</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2020</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>liuyang-70d12d</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.2.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.3.0</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">Visitors</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">Views</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now = new Date();
function createtime() {
var grt= new Date("11/09/2020 00:00:00");
now.setTime(now.getTime()+250);
days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
document.getElementById("timeDate").innerHTML = "我已来到这个世界 "+dnum+" 天 ";
document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
}
setInterval("createtime()",250);</script></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css"><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="/js/utils.js?v=2.3.0"></script><script src="/js/stun-boot.js?v=2.3.0"></script><script src="/js/scroll.js?v=2.3.0"></script><script src="/js/header.js?v=2.3.0"></script><script src="/js/sidebar.js?v=2.3.0"></script></body></html>