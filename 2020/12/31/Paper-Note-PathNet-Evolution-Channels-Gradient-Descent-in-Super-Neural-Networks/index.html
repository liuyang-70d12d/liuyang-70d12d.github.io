<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/assets/favicon/favicon_01/favicon-16x16.png?v=2.3.0" type="image/png" sizes="16x16"><link rel="icon" href="/assets/favicon/favicon_01/favicon-32x32.png?v=2.3.0" type="image/png" sizes="32x32"><meta name="description" content="PathNet: Evolution Channels Gradient Descent in Super Neural Networks Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra arXiv">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Note: PathNet: Evolution Channels Gradient Descent in Super Neural Networks">
<meta property="og:url" content="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/index.html">
<meta property="og:site_name" content="Hello, Stranger">
<meta property="og:description" content="PathNet: Evolution Channels Gradient Descent in Super Neural Networks Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra arXiv">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/01.png">
<meta property="article:published_time" content="2020-12-31T03:09:16.000Z">
<meta property="article:modified_time" content="2021-01-01T14:22:27.602Z">
<meta property="article:author" content="liuyang-70d12d">
<meta property="article:tag" content="Continual Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/01.png"><title>Paper Note: PathNet: Evolution Channels Gradient Descent in Super Neural Networks | Hello, Stranger</title><link ref="canonical" href="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.3.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.2.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Paper Note: PathNet: Evolution Channels Gradient Descent in Super Neural Networks</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2020-12-31</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2021-01-01</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">Visited</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"><p><font face="Adobe Caslon Pro"><strong>PathNet: Evolution Channels Gradient Descent in Super Neural<br>
Networks</strong></font></p>
<p><font face="Adobe Caslon Pro"><em>Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra</em></font></p>
<p><font face="Adobe Caslon Pro"><em>arXiv:1701.08734</em> </font></p>
<p><font face="Adobe Caslon Pro">[<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.08734">paper</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>]</font> <font face="Adobe Caslon Pro">[<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://github.com/jsikyoon/pathnet">code_1</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>]</font> <font face="Adobe Caslon Pro">[<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://github.com/kimhc6028/pathnet-pytorch">code_2</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>]</font></p>
<a id="more"></a> 

        <h2 id="font-faceadobe-caslon-promain-ideafont">
          <a href="#font-faceadobe-caslon-promain-ideafont" class="heading-link"><i class="fas fa-link"></i></a>Main Idea</h2>
      
<p><font face="Adobe Caslon Pro"><strong>文章讨论了PathNet在监督学习和强化学习两个case 下的应用。</strong> 但是这篇blog重点只看了监督学习方面的内容。论文其实举了两个PathNet 的例子，在监督学习方面是举得持续学习的例子，在论文里叫串行实现（serial implementation），在强化学习方面举得是并行实现的例子（parallel implementation）。</font></p>
<p><font face="Adobe Caslon Pro">从 Figure 1 说起：模型要学习的第一个任务是 Pong，第二个任务是 Alien。两个任务都分别训练 80M timesteps。Box 1中的紫色的线展现了在Pong训练开始时，在网络模型中64个随机初始化的路径。然后使用遗传算法中的锦标赛选择法来演进（evolve）路径。在进行 fitness 评估的时候，路径是使用强化学习算法来训练若干个episodes。因此演进（evolution）和学习（learning）是同时发生的，演进只是指明了应该改变哪里的weight 和 bias。Box 2 和 Box 3 展示了路径的收敛过程，直至 Box 4 第一个任务训练结束时，最终收敛至的“胜出”的 pathway。然后将对于 Pong 任务的最优路径固定住，也就是在那条 path 上的 modules 的 weights 和 biases 全部固定住，开始训练 task B。Box 5 用深红色的线表示被固定的path，用浅蓝色的线表示对于第二个任务所随机初始化的若干 path。过程像第一个任务时一样，对于 Alien 任务的最优路径也会被固定住，如 Box 9 中的深蓝色线条所示。</font></p>
<p><font face="Adobe Caslon Pro"></font></p>
<img src="/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/01.png" class>
<p><font face="Adobe Caslon Pro">Figure 1.</font></p>

        <h2 id="font-faceadobe-caslon-promethodfont">
          <a href="#font-faceadobe-caslon-promethodfont" class="heading-link"><i class="fas fa-link"></i></a>Method</h2>
      

        <h3 id="font-faceadobe-caslon-propathnet-architecturefont">
          <a href="#font-faceadobe-caslon-propathnet-architecturefont" class="heading-link"><i class="fas fa-link"></i></a>PathNet Architecture</h3>
      
<p><font face="Adobe Caslon Pro">PathNet 是一个模块化（modular）的深度神经网络，包括 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 层，每一层有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 个 modules。每个 module 本身是一个神经网络，即卷积网络或全连接网络，后面接着 transfer function（这里用的是 ReLU）。对于每一层而言，该层 modules 的输出在传递给后面一层的 active modules 之前，首先进行求和。一个module如果存在于 the path genotype currently being evaluated，那就称其为 <em>active</em> 的。在一条路径（pathway）中每一层最多允许存在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> 个不同的 modules（一般地，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>3</mn><mtext> </mtext><mrow><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mrow><mtext> </mtext><mn>4</mn></mrow></mrow><annotation encoding="application/x-tex">N=3\ \rm{or}\ 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mspace"> </span><span class="mord"><span class="mord"><span class="mord mathrm">o</span><span class="mord mathrm">r</span></span><span class="mspace"> </span><span class="mord mathrm">4</span></span></span></span></span> ）。网络的最后一层对于每个任务而言是不共享的且是独特的。对于监督学习的case而言，最后一层就是对应于不同任务的 linear layer。</font></p>

        <h3 id="font-faceadobe-caslon-propathway-evolution-serial-and-parallelfont">
          <a href="#font-faceadobe-caslon-propathway-evolution-serial-and-parallelfont" class="heading-link"><i class="fas fa-link"></i></a>Pathway Evolution: Serial and Parallel</h3>
      
<p><font face="Adobe Caslon Pro">P 个 genotypes（pathways）被随机初始化，每个 genotypes 是一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">N\times L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 的整数矩阵，该矩阵指示了在该 pathway 中每一层活跃的 modules。在序列监督任务中（serial supervised implementation），使用了一个 binary tournament selection algorithm。这里 Tournament Selection（锦标赛选择法） 是 Genetic Algorithm（遗传算法）的一种[<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Tournament_selection">参考1-Wikipedia</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>] [<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/tournament-selection-ga/">参考2 - GeeksforGeeks</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>]。 </font></p>
<p><font face="Adobe Caslon Pro">这个锦标赛选择法具体是怎么用的呢？如下：首先随机出一个 genotype（注意，前面说过genotype实际上就是一个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">N\times L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 的矩阵），然后训练 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> 轮它的 pathway，它的 fitness 是训练过程中的 negative classication error。然后再随机出另一个 genotype，也是训练 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span></span></span></span> 轮它的 pathway。然后用 winning pathway 的 genotype 去覆写 losing pathway 的 genotype，并在此基础上，对 genotype 的每一个元素都有：以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mo stretchy="false">[</mo><mi>N</mi><mo>×</mo><mi>L</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">1/[N\times L]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mclose">]</span></span></span></span> 的概率加上一个范围在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-2,2]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mclose">]</span></span></span></span> 的整数。A local neighbourhood was used to promote spatial localization of network functionality.（这句没搞懂它在说什么……）</font></p>

        <h3 id="font-faceadobe-caslon-protransfer-learning-paradigmfont">
          <a href="#font-faceadobe-caslon-protransfer-learning-paradigmfont" class="heading-link"><i class="fas fa-link"></i></a>Transfer Learning Paradigm</h3>
      
<p><font face="Adobe Caslon Pro">一旦任务 A 被训练了一定的时长或者达到了某个 error 阈值，fit 得最好的 pathway 就会被固定住，也就是说它的参数不会再发生变化。不在该最佳路径当中的其它参数将会被重新初始化。（我们发现没有重新初始化的迁移的表现并不比 fine-tuning 的表现好。）然后就又会有新的随机的pathways被初始化，然后在 task B 上演进（evolved）/训练（trained）。</font></p>
<p><font face="Adobe Caslon Pro"></font></p>
<p><font face="Adobe Caslon Pro"></font></p>
<p><font face="Adobe Caslon Pro"></font></p>
<p><font face="Adobe Caslon Pro"></font></p>

        <h2 id="font-faceadobe-caslon-proconclusion-font">
          <a href="#font-faceadobe-caslon-proconclusion-font" class="heading-link"><i class="fas fa-link"></i></a>Conclusion</h2>
      
<p><font face="Adobe Caslon Pro">对于 PathNet 而言，网络的 weights 和 biases 是通过梯度下降学习得到的，而演进（evolution）决定了要去训练哪一部分参数。</font></p>
<p><font face="Adobe Caslon Pro">PathNet 可以看作是“可演进的 dropout”的一种实现，在这种实现中 dropout 不再是随机地 drop 掉一些units和它们的连接，而是可以演进的。</font></p>
<p><font face="Adobe Caslon Pro">论文最后的 Conclusion 还巴拉巴拉说了好多，粗扫了一眼，没有很感兴趣的。</font></p>
<p><font face="Adobe Caslon Pro"><strong>2021年的第一篇 Blog，完成！</strong></font></p>
<center><font face="楷体" color="grey"> 小舟从此逝，江海寄余生。</font></center></div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="https://hello-liuyang.com">liuyang-70d12d</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/">https://hello-liuyang.com/2020/12/31/Paper-Note-PathNet-Evolution-Channels-Gradient-Descent-in-Super-Neural-Networks/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://hello-liuyang.com/tags/Continual-Learning/">Continual Learning</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/01/05/%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%B7%A5%E4%B8%9A%E4%BA%92%E8%81%94%E7%BD%91/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">数据：工业互联网</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2020/12/30/Paper-Note-Overcoming-Catastrophic-Forgetting-with-Hard-Attention-to-the-Task/"><span class="paginator-prev__text">Paper Note: Overcoming Catastrophic Forgetting with Hard Attention to the Task</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#font-faceadobe-caslon-promain-ideafont"><span class="toc-number">1.</span> <span class="toc-text">
          Main Idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#font-faceadobe-caslon-promethodfont"><span class="toc-number">2.</span> <span class="toc-text">
          Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#font-faceadobe-caslon-propathnet-architecturefont"><span class="toc-number">2.1.</span> <span class="toc-text">
          PathNet Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#font-faceadobe-caslon-propathway-evolution-serial-and-parallelfont"><span class="toc-number">2.2.</span> <span class="toc-text">
          Pathway Evolution: Serial and Parallel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#font-faceadobe-caslon-protransfer-learning-paradigmfont"><span class="toc-number">2.3.</span> <span class="toc-text">
          Transfer Learning Paradigm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#font-faceadobe-caslon-proconclusion-font"><span class="toc-number">3.</span> <span class="toc-text">
          Conclusion</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/avatar/avatar.png" alt="avatar"></div></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="mailto:ly_liuyang19@qq.com" target="_blank" rel="noopener" data-popover="social.mail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">Email</span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">45</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">18</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>liuyang-70d12d</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.2.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.3.0</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">Visitors</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">Views</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div><div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now = new Date();
function createtime() {
var grt= new Date("11/09/2020 00:00:00");
now.setTime(now.getTime()+250);
days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
document.getElementById("timeDate").innerHTML = "我已来到这个世界 "+dnum+" 天 ";
document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
}
setInterval("createtime()",250);</script></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.css" rel="stylesheet" type="text/css"><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="/js/utils.js?v=2.3.0"></script><script src="/js/stun-boot.js?v=2.3.0"></script><script src="/js/scroll.js?v=2.3.0"></script><script src="/js/header.js?v=2.3.0"></script><script src="/js/sidebar.js?v=2.3.0"></script></body></html>